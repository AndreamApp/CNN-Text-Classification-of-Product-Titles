
    train_mode = 'WORD-NON-STATIC'     # 训练模式，'CHAR'为字符级，样本分割为字符并使用自训练词嵌入
                            # 'WORD'为词级，样本分词并使用word2vec预训练的词向量
                            # SGNS_WORD_NGRAM

    class_num = 1258        # 输出类别的数目
    embedding_dim = 128      # 词向量维度，'CHAR'模式适用，
                            # 'WORD'模式默认为preprocess.py中定义的vec_dim

    filter_num = 300        # 卷积核数目
    filter_sizes = [1, 2, 3, 4, 5]         # 卷积核尺寸
    vocab_size = preprocess.VOCAB_SIZE      # 词汇表大小

    dense_unit_num = 512        # 全连接层神经元

    dropout_keep_prob = 0.5     # dropout保留比例（弃用）
    learning_rate = 1e-3    # 学习率

    train_batch_size = 128         # 每批训练大小
    valid_batch_size = 5000       # 每批验证大小
    test_batch_size = 5000        # 每批测试大小
    valid_per_batch = 500           # 每多少批进行一次验证
    epoch_num = 25001        # 总迭代轮次
valid 90.56%

    train_mode = 'CHAR-RANDOM'     # 训练模式，'CHAR-RANDOM'为字符级，随机初始化词向量并训练优化
                                        # 'WORD'为词级，使用word2vec预训练的词向量
                                        # 'WORD-NON-STATIC'同'WORD', 但是词向量能够继续在训练中优化
    class_num = 1258        # 输出类别的数目
    embedding_dim = 128      # 词向量维度，仅'CHAR-RANDOM'模式适用，
                            # 'WORD'模式默认为preprocess.py中定义的vec_dim

    filter_num = 300        # 卷积核数目
    filter_sizes = [2, 3, 4, 5, 6]         # 卷积核尺寸
    vocab_size = preprocess.VOCAB_SIZE      # 词汇表大小

    dense_unit_num = 512        # 全连接层神经元

    dropout_keep_prob = 0.5     # dropout保留比例（弃用）
    learning_rate = 1e-3    # 学习率

    train_batch_size = 128         # 每批训练大小
    valid_batch_size = 3000       # 每批验证大小
    test_batch_size = 5000        # 每批测试大小
    valid_per_batch = 500           # 每多少批进行一次验证
    epoch_num = 30001        # 总迭代轮次
valid 94.87%